# Description

- 7 hours of study material

This course provides a theoretical and applied overview of modern Artificial Intelligence systems, with a primary focus on Large Language Models (LLMs), Agentic AI, and Transformer-based architectures. It is structured to integrate foundational theory with guided laboratory activities, ensuring a clear connection between underlying concepts and their practical implementation.

The program examines the training and operational principles of LLMs, including prompt engineering methodologies and Retrieval-Augmented Generation (RAG). Practical sessions address the design and implementation of RAG pipelines using established frameworks such as LangChain and LlamaIndex. The course further introduces Agentic AI, covering the architectural components and development patterns of autonomous AI agents, supported by hands-on laboratories in Python and the use of modern agent frameworks including LangGraph, CrewAI, and AutoGen, applied to representative real-world scenarios.

The final modules are dedicated to Transformer architectures and the Hugging Face ecosystem, providing theoretical insight into self-attention mechanisms, encoderâ€“decoder models, and sequence-to-sequence paradigms. These concepts are reinforced through applied laboratories focused on fine-tuning transformer models for Natural Language Processing tasks, including text classification, Named Entity Recognition (NER), translation, and summarization.

## Chapters

1. Introduction to the Course

2. **[Theory]** Large language models
   * what is a LLM
   * GTP2, GPT3 toward ChatGPT
   * how a LLM is trained
   * OpenAI LLMs
   * Tecniques for prompt engineering
   * LLM - multitasking
   * RAG and their uses
   * components of a RAG pipeline
   * Create a RAG with Llamaindex/LangChain
3. **[Practice]** Large language models
    * chat with OpenAI
    * introduction to LangChain
    * Pipeline for text Generation
    * llamaindex
    * Setuo of a vectorial database
4. **[Theory]** Agentic AI
    * Agentic AI
    * Introduction to Deep Learning per Agentic AI
    * Fundamenltals components of an AI agent
    * Design patterns and architectures for AI agents
    * Framework for development of AI agents
    * Techinques of training for AI agents: strategies of LLm prompting and role playing
    * Fine-tuning of small LLMs for specific tasks
    * booking systems, a use case
5. **[Practice]** Agentic AI
    * implementazione di un AI agent da zero in Python
    * Utilizzare LangGraph per appliccazioni di agentic AI
    * Utilizzare CrewAI per applicazioni di AgenticAI
    * Utilizzare AutoGen per applicazioni di Agentic AI
    * Booking system
6. **[Theory]** Transformers and Huggingface architectures
    * History and Fundamental Concepts of Transformers
    * Encoders and Decoders: Roles and Structure
    * Self-Attention Mechanism and Multi-Head Attention
    * Introduction to the Hugging Face Library
    * Fine-Tuning and Transfer Learning Concepts
    * Introduction to Text Generation
    * Transformer Architectures and Named Entity Recognition (NER)
    * Sequence-to-Sequence Models and Applications in Transformers
7. **[Practice]** Transformers for NLP
    * Fine-Tuning for Text Classification
    * Named Entity Recognition (NER)
    * Translation and Summarization with Transformers