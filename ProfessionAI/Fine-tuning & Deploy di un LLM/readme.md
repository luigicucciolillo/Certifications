# Fine-Tuning and Deployment of a Large Language Model (LLM)

Date: 26/06/2025
Duration: 2 hours

A two-hour masterclass titled "Fine-Tuning and Deployment of a Large Language Model (LLM)", aimed at providing a practical, end-to-end overview of adapting and serving transformer-based models.

The session focused on fine-tuning pre-trained models using the Hugging Face Transformers library and executing training workflows in Google Colab. We covered dataset formatting, tokenization, training loop configuration, evaluation, and checkpoint management.

In the second part, we explored deployment strategies using Hugging Face Spaces, with emphasis on reproducibility, interface integration (via Gradio), and model versioning.

This hands-on session was designed for engineers, data scientists, and applied researchers seeking to operationalize custom LLMs efficiently using open-source tools and cloud-based resources.